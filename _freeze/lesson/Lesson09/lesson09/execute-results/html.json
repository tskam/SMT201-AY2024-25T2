{
  "hash": "5cf3248fed7f7094ef57849734b608af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lesson 9: Applied Image Analysis for the Urban Environment\"\nauthor: \"Dr. Kam Tin Seong<br/>Assoc. Professor of Information Systems\"\ninstitute: \"School of Computing and Information Systems,<br/>Singapore Management University\"\ndate: \"last-modified\"\ndate-modified: \"last-modified\" \nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    show-notes: false\n    slide-number: true\n    show-slide-number: 'all'\neditor: visual\nexecute: \n  freeze: true\n---\n\n\n\n## Content\n\n-   Urban Land Cover Classification\n\n-   Unsupervised Classification Methods\n\n-   Supervised Classification Process\n\n-   Classification Accuracy Metrics\n\n## Urban Land Cover Classification of Satellite Remote Sensing Imagery\n\n::::: columns\n::: {.column width=\"50%\"}\nThe process of assigning a pixel (or groups of pixels) of remote sensing image to an urban land cover class.\n\n-   **Unsupervised Classification** in which the computer assigns pixels to categories with no instructions from the image analyst.\n\n-   **Supervised Classification** in which the image analyst provides training-site information that the computer uses to assign pixels to categories.\n:::\n\n::: {.column width=\"50%\"}\n![](img/image1.jpg)\n:::\n:::::\n\n## Unsupervised Classification\n\n::::: columns\n::: {.column width=\"50%\"}\n-   The goal of unsupervised classification is to automatically segregate pixels of a remote sensing image into groups of similar spectral character.\n\n-   Classification is done using one of several statistical routines generally called \"clustering\" where classes of pixels are created based on their shared spectral signatures.\n\n-   Clusters are split and /or merged until further clustering doesn't improve the explanation of the variation in the scene.\n:::\n\n::: {.column width=\"50%\"}\n![](img/image2.jpg)\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Unsupervised classification algirithm\n\n-   Hierarchical clustering\n-   k-means clustering\n-   ISODATA (iterative Self-Organizing Data Analysis Technique)\n-   fuzzy-clustering\n\n------------------------------------------------------------------------\n\n### k-means clustering\n\n-   Partitioning clustering approach.\n\n-   Each cluster is associated with a centroid (center point).\n\n-   Each point is assigned to the cluster with the closest centroid.\n\n-   Number of clusters, K, must be specified.\n\n-   The basic algorithm is very simple\n\n![](img/image3.jpg){width=\"438\"}\n\n------------------------------------------------------------------------\n\n### How the K-means algorithm works?\n\nThe clustering process starts by randomly assigning objects to a number of clusters. The objects are then successively reassigned to other clusters to minimize the within-cluster variation, which is basically the (squared) distance from each observation to the center of the associated cluster. If the reallocation of an object to another cluster decreases the within-cluster variation, this object is reassigned to that cluster.\n\n![](img/image4.jpg){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### ISODATA clustering algorithm\n\n-   The ISODATA algorithm has some further refinements by splitting and merging of clusters (JENSEN, 1996).\n\n    -   Clusters are merged if either the number of members (pixel) in a cluster is less than a certain threshold or if the centers of two clusters are closer than a certain threshold.\n\n    -   Clusters are split into two different clusters if the cluster standard deviation exceeds a predefined value and the number of members (pixels) is twice the threshold for the minimum number of members.\n\n-   The ISODATA algorithm is similar to the k-means algorithm with the distinct difference that the ISODATA algorithm allows for different number of clusters while the k-means assumes that the number of clusters is known a priori.\n\n------------------------------------------------------------------------\n\n### ISODATA Iteration\n\n-   In ISODATA iterations, pixels assigned to clusters with closest spectral mean; mean recalculated; pixels reassigned.\n\n![](img/image6.jpg){fig-align=\"center\"}\n\n-   Continues until maximum iterations or convergence threshold reached.\n\n------------------------------------------------------------------------\n\n### Clustering Parameters\n\n::::: columns\n::: {.column width=\"50%\"}\n-   To perform ISODATA clustering; NTM\n    -   N -- maximum number of clusters to be considered. Since each cluster is the basis for a class, this number becomes the maximum number of classes to be formed. The ISODATA process begins by determining N arbitrary cluster means. Some clusters with too few pixels can be eliminated, leaving less than N clusters.\n    -   T -- a convergence threshold, which is the maximum percentage of pixels whose class values are allowed to be unchanged between iterations.\n    -   M -- maximum number of iterations to be performed\n:::\n\n::: {.column width=\"50%\"}\n-   Number of clusters: 10 to 15 per desired land cover class.\n\n-   Convergence threshold: percentage of pixels whose class values should not change between iterations; generally set to 95%.\n\n-   Maximum number of iterations: ideally, the convergence threshold should be reached. Should set \"reasonable\" parameters so that convergence is reached before iterations run out.\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Limitation of k-means and ISODATA clustering\n\nk-means and ISODATA clustering work best for images with clusters that are spherical and that have the same variance. This is often not true for remote sensing images. For example, a cluster with \"desert\" pixels is compact/circular. A \"forest\" cluster, however, is usually more or less elongated/oval with a much larger variability compared to the \"desert\" cluster. While the \"desert\" cluster is usually very well detected by the k-means algorithm as one distinct cluster, the \"forest\" cluster is often split up into several smaller cluster. The way the \"forest\" cluster is split up can vary quite a bit for different starting values and is thus arbitrary.\n\n![](img/image7.jpg){fig-align=\"center\"}\n\n## Supervised Classification\n\n::::: columns\n::: {.column width=\"50%\"}\nSupervised training is closely controlled by the analyst. In this process, you select pixels that represent patterns or land cover features that you recognize, or that you can identify with help from other sources, such as aerial photos, ground truth data, or maps. Knowledge of the data, and of the classes desired, is required before classification.\n\nBy identifying patterns, you can instruct the computer system to identify pixels with similar characteristics. If the classification is accurate, the resulting classes represent the categories within the data that you originally identified.\n:::\n\n::: {.column width=\"50%\"}\n![](img/image8.jpg)\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Statistical Learning: Maximum Likelihood Algorithm\n\n::::: columns\n::: {.column width=\"50%\"}\nMaximum likelihood classification assumes that the statistics for each class in each band are normally distributed and calculates the probability that a given pixel belongs to a specific class. Unless you select a probability threshold, all pixels are classified. Each pixel is assigned to the class that has the highest probability (that is, the maximum likelihood). If the highest probability is smaller than a threshold you specify, the pixel remains unclassified.\n:::\n\n::: {.column width=\"50%\"}\nThe discriminant function, described by Richards and Jia (2006), is calculated for every pixel as:\n\n![](img/image17.jpg)\n\n![](img/image17b.jpg)\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Statistical Learning: Minimum Distance Algorithm\n\n::::: columns\n::: {.column width=\"50%\"}\nThe minimum distance technique uses the mean vectors of each endmember and calculates the Euclidean distance from each unknown pixel to the mean vector for each class. All pixels are classified to the nearest class unless a standard deviation or distance threshold is specified, in which case some pixels may be unclassified if they do not meet the selected criteria.\n:::\n\n::: {.column width=\"50%\"}\nThe distance is calculated for every pixel in the image, assigning the class of the spectral signature that is closer, according to the following discriminant function (adapted from Richards and Jia, 2006):\n\n![](img/image18a.jpg) ![](img/image18b.jpg)\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Statistical Learning: Spectral Angle Mapping\n\n::::: columns\n::: {.column width=\"50%\"}\nThe Spectral Angle Mapping calculates the spectral angle between spectral signatures of image pixels and training spectral signatures. The spectral angle Î¸ is defined as (Kruse et al., 1993):\n\n![](img/image19a.jpg) ![](img/image19b.jpg)\n:::\n\n::: {.column width=\"50%\"}\nTherefore a pixel belongs to the class having the lowest angle, that is:\n\n![](img/image20a.jpg){width=\"550\"} ![](img/image20b.jpg){width=\"480\"} ![](img/image20c.jpg){width=\"500\"}\n:::\n:::::\n\n------------------------------------------------------------------------\n\n## Machine Learning: Random Forest\n\n:::::: columns\n:::: {.column width=\"50%\"}\n::: {style=\"font-size: 0.8em\"}\n-   The Random Forest (RF) algorithm (Breimann 2001) is a supervised classification algorithm. It builds upon the concept of decision trees (DT).\n-   The RF relies on many self-learning decision trees (i.e. \"Forest\"). The idea behind using many decision trees (i.e. an ensemble) is that many base learners can come to one strong and robust decision compared to a single DT.\n-   Different from the manual (expert-based) definition of decision rules, the RF uses self-learning decision trees. These trees automatically define rules at each node based on a training dataset.\n-   RF seeks to minimize the heterogeneity of the two resulting subsets of the data created by the respective rule. Heterogeneity is in this case expressed as the Gini impurity index and the rule creating the least heterogeneous subsets of the data is used for the respective node.\n:::\n::::\n\n::: {.column width=\"50%\"}\n![](img/image21.jpg){width=\"640\"}\n:::\n::::::\n\n------------------------------------------------------------------------\n\n### Accuracy assessment for classifications\n\nThe basic principle for all accuracy assessment is to compare estimates with reality, and to quantify the difference between the two.\n\n-   In the context of remote sensing-based land cover classifications, the 'estimates' are the classes mapped for each pixel, and 'reality' is the actual land cover in the areas corresponding to each pixel.\n\n------------------------------------------------------------------------\n\n### The confusion matrix\n\nA confusion matrix (or error matrix) is usually used as the quantitative method of characterising image classification accuracy. It is a table that shows correspondence between the classification result and a reference image. I.e., to create the confusion matrix we need the ground truth data, such as cartographic information, results of manually digitizing an image, field work/ground survey results recorded with a GPS-receiver.\n\n![](img/image9.jpg){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### User, producer, and overall accuracy\n\n::: {style=\"font-size: 0.8em\"}\n-   The user's accuracy column shows false positives, or errors of commission, where pixels are incorrectly classified as a known class when they should have been classified as something different. User's accuracy is also referred to as Type 1 error. The data to compute this error rate is read from the rows of the table. The user's accuracy is calculated by dividing the total number of classified points that agree with the reference data by the total number of classified points for that class. The Total row shows the number of points that should have been identified as a given class, according to the reference data.\n:::\n\n![](img/image10.jpg){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### User, producer, and overall accuracy\n\n::: {style=\"font-size: 0.8em\"}\n-   The producer's accuracy column shows false negatives, or errors of omission. The producer's accuracy indicates how accurately the classification results meet the expectation of the creator. Producer's accuracy is also referred to as Type 2 error. The data to compute this error rate is read in the columns of the table. The producer's accuracy is calculated by dividing the total number of classified points that agree with reference data by the total number of reference points for that class. These values are false-negative values within the classified results. The Total column shows the number of points that were identified as a given class, according to the classified map.\n:::\n\n![](img/image11.jpg){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### User, producer, and overall accuracy\n\n::: {style=\"font-size: 0.8em\"}\n-   The **overall accuracy** answers the following question: 'What proportion of the map is correctly classified?', which can often be interpreted simply as 'how accurate is the map?'. Looking at the values in the diagonal of the confusion matrix in the confusion matrix below, these sum up to 49+40+59=148, out of a total 166 pixels in the validation data set. 148 out of 166 is 89.16%, so based on the validation data we estimate that 89.16% of the map is correctly classified.\n:::\n\n![](img/image12.jpg){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Kappa\n\n::::: columns\n::: {.column width=\"50%\"}\nKAPPA analysis is a discrete multivariate technique used in accuracy assessments.\n\n-   It yields a Khat statistic (an estimate of KAPPA) that is a measure of agreement or accuracy.\n\n-   The Khat statistic is computed by using the formula below:\n\n![](img/image14.jpg)\n:::\n\n::: {.column width=\"50%\"}\nRating criteria of Kappa statistics\n\n![](img/image15.jpg){width=\"589\"}\n\n![](img/image15.jpg){width=\"2\" height=\"1\"}\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### F1 Score\n\n::::: columns\n::: {.column width=\"50%\"}\n-   The goal of a good image analysis is, of course, to have a large number of True Presences, and a small number of False Presences and a small number of False Negatives.\n\n-   To quantify how well the image analysis succeeded in this, the value typically calculated is called the F1 score, which is calculated as:\n\n![](img/image16a.jpg){fig-align=\"center\" width=\"423\"}\n:::\n\n::: {.column width=\"50%\"}\n![](img/image16.jpg)\n\n-   The F1 score has the nice property of having values that range from 0 (worst) to 1 (best), which makes it easy to interpret.\n:::\n:::::\n\n## Reference\n\n-   [Land Cover & Land Use](https://www.nrcan.gc.ca/maps-tools-and-publications/satellite-imagery-and-air-photos/tutorial-fundamentals-remote-sensing/educational-resources-applications/land-cover-biomass-mapping/land-cover-land-use/9373)\n\n-   Richards and Jia (2006) **Remote Sensing Digital Image Analysis: An Introduction**, Springer.\n\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}