<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Dr.&nbsp;Kam Tin Seong Assoc. Professor of Information Systems">
  <meta name="dcterms.date" content="2025-03-23">
  <title>SMT201-AY2024-25T2 – Lesson 10: Urban Land Cover Classification and Change Detection with Remotely Sensed Data</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lesson 10: Urban Land Cover Classification and Change Detection with Remotely Sensed Data</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;Kam Tin Seong<br>Assoc. Professor of Information Systems 
</div>
        <p class="quarto-title-affiliation">
            School of Computing and Information Systems,<br>Singapore Management University
          </p>
    </div>
</div>

  <p class="date">2025-03-23</p>
</section>
<section id="content" class="slide level2">
<h2>Content</h2>
<ul>
<li><p>Urban Land Cover Classification</p></li>
<li><p>Unsupervised Classification Methods</p></li>
<li><p>Supervised Classification Process</p></li>
<li><p>Classification Accuracy Metrics</p></li>
</ul>
</section>
<section id="urban-land-cover-classification-of-satellite-remote-sensing-imagery" class="slide level2">
<h2>Urban Land Cover Classification of Satellite Remote Sensing Imagery</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The process of assigning a pixel (or groups of pixels) of remote sensing image to an urban land cover class.</p>
<ul>
<li><p><strong>Unsupervised Classification</strong> in which the computer assigns pixels to categories with no instructions from the image analyst.</p></li>
<li><p><strong>Supervised Classification</strong> in which the image analyst provides training-site information that the computer uses to assign pixels to categories.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image1.jpg"></p>
</div></div>
</section>
<section id="unsupervised-classification" class="slide level2">
<h2>Unsupervised Classification</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The goal of unsupervised classification is to automatically segregate pixels of a remote sensing image into groups of similar spectral character.</p></li>
<li><p>Classification is done using one of several statistical routines generally called “clustering” where classes of pixels are created based on their shared spectral signatures.</p></li>
<li><p>Clusters are split and /or merged until further clustering doesn’t improve the explanation of the variation in the scene.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image2.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="unsupervised-classification-algirithm">Unsupervised classification algirithm</h3>
<ul>
<li>Hierarchical clustering</li>
<li>k-means clustering</li>
<li>ISODATA (iterative Self-Organizing Data Analysis Technique)</li>
<li>fuzzy-clustering</li>
</ul>
</section>
<section class="slide level2">

<h3 id="k-means-clustering">k-means clustering</h3>
<ul>
<li><p>Partitioning clustering approach.</p></li>
<li><p>Each cluster is associated with a centroid (center point).</p></li>
<li><p>Each point is assigned to the cluster with the closest centroid.</p></li>
<li><p>Number of clusters, K, must be specified.</p></li>
<li><p>The basic algorithm is very simple</p></li>
</ul>

<img data-src="img/image3.jpg" width="438" class="r-stretch"></section>
<section class="slide level2">

<h3 id="how-the-k-means-algorithm-works">How the K-means algorithm works?</h3>
<p>The clustering process starts by randomly assigning objects to a number of clusters. The objects are then successively reassigned to other clusters to minimize the within-cluster variation, which is basically the (squared) distance from each observation to the center of the associated cluster. If the reallocation of an object to another cluster decreases the within-cluster variation, this object is reassigned to that cluster.</p>

<img data-src="img/image4.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="isodata-clustering-algorithm">ISODATA clustering algorithm</h3>
<ul>
<li><p>The ISODATA algorithm has some further refinements by splitting and merging of clusters (JENSEN, 1996).</p>
<ul>
<li><p>Clusters are merged if either the number of members (pixel) in a cluster is less than a certain threshold or if the centers of two clusters are closer than a certain threshold.</p></li>
<li><p>Clusters are split into two different clusters if the cluster standard deviation exceeds a predefined value and the number of members (pixels) is twice the threshold for the minimum number of members.</p></li>
</ul></li>
<li><p>The ISODATA algorithm is similar to the k-means algorithm with the distinct difference that the ISODATA algorithm allows for different number of clusters while the k-means assumes that the number of clusters is known a priori.</p></li>
</ul>
</section>
<section class="slide level2">

<h3 id="isodata-iteration">ISODATA Iteration</h3>
<ul>
<li>In ISODATA iterations, pixels assigned to clusters with closest spectral mean; mean recalculated; pixels reassigned.</li>
</ul>

<img data-src="img/image6.jpg" class="quarto-figure quarto-figure-center r-stretch"><ul>
<li>Continues until maximum iterations or convergence threshold reached.</li>
</ul>
</section>
<section class="slide level2">

<h3 id="clustering-parameters">Clustering Parameters</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>To perform ISODATA clustering; NTM
<ul>
<li>N – maximum number of clusters to be considered. Since each cluster is the basis for a class, this number becomes the maximum number of classes to be formed. The ISODATA process begins by determining N arbitrary cluster means. Some clusters with too few pixels can be eliminated, leaving less than N clusters.</li>
<li>T – a convergence threshold, which is the maximum percentage of pixels whose class values are allowed to be unchanged between iterations.</li>
<li>M – maximum number of iterations to be performed</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Number of clusters: 10 to 15 per desired land cover class.</p></li>
<li><p>Convergence threshold: percentage of pixels whose class values should not change between iterations; generally set to 95%.</p></li>
<li><p>Maximum number of iterations: ideally, the convergence threshold should be reached. Should set “reasonable” parameters so that convergence is reached before iterations run out.</p></li>
</ul>
</div></div>
</section>
<section class="slide level2">

<h3 id="limitation-of-k-means-and-isodata-clustering">Limitation of k-means and ISODATA clustering</h3>
<p>k-means and ISODATA clustering work best for images with clusters that are spherical and that have the same variance. This is often not true for remote sensing images. For example, a cluster with “desert” pixels is compact/circular. A “forest” cluster, however, is usually more or less elongated/oval with a much larger variability compared to the “desert” cluster. While the “desert” cluster is usually very well detected by the k-means algorithm as one distinct cluster, the “forest” cluster is often split up into several smaller cluster. The way the “forest” cluster is split up can vary quite a bit for different starting values and is thus arbitrary.</p>

<img data-src="img/image7.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="supervised-classification" class="slide level2">
<h2>Supervised Classification</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Supervised training is closely controlled by the analyst. In this process, you select pixels that represent patterns or land cover features that you recognize, or that you can identify with help from other sources, such as aerial photos, ground truth data, or maps. Knowledge of the data, and of the classes desired, is required before classification.</p>
<p>By identifying patterns, you can instruct the computer system to identify pixels with similar characteristics. If the classification is accurate, the resulting classes represent the categories within the data that you originally identified.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image8.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="statistical-learning-maximum-likelihood-algorithm">Statistical Learning: Maximum Likelihood Algorithm</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>Maximum likelihood classification assumes that the statistics for each class in each band are normally distributed and calculates the probability that a given pixel belongs to a specific class. Unless you select a probability threshold, all pixels are classified. Each pixel is assigned to the class that has the highest probability (that is, the maximum likelihood). If the highest probability is smaller than a threshold you specify, the pixel remains unclassified.</p>
</div><div class="column" style="width:50%;">
<p>The discriminant function, described by Richards and Jia (2006), is calculated for every pixel as:</p>
<p><img data-src="img/image17.jpg"></p>
<p><img data-src="img/image17b.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="statistical-learning-minimum-distance-algorithm">Statistical Learning: Minimum Distance Algorithm</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The minimum distance technique uses the mean vectors of each endmember and calculates the Euclidean distance from each unknown pixel to the mean vector for each class. All pixels are classified to the nearest class unless a standard deviation or distance threshold is specified, in which case some pixels may be unclassified if they do not meet the selected criteria.</p>
</div><div class="column" style="width:50%;">
<p>The distance is calculated for every pixel in the image, assigning the class of the spectral signature that is closer, according to the following discriminant function (adapted from Richards and Jia, 2006):</p>
<p><img data-src="img/image18a.jpg"> <img data-src="img/image18b.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="statistical-learning-spectral-angle-mapping">Statistical Learning: Spectral Angle Mapping</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The Spectral Angle Mapping calculates the spectral angle between spectral signatures of image pixels and training spectral signatures. The spectral angle θ is defined as (Kruse et al., 1993):</p>
<p><img data-src="img/image19a.jpg"> <img data-src="img/image19b.jpg"></p>
</div><div class="column" style="width:50%;">
<p>Therefore a pixel belongs to the class having the lowest angle, that is:</p>
<p><img data-src="img/image20a.jpg" width="550"> <img data-src="img/image20b.jpg" width="480"> <img data-src="img/image20c.jpg" width="500"></p>
</div></div>
</section>
<section id="machine-learning-random-forest" class="slide level2">
<h2>Machine Learning: Random Forest</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 0.8em">
<ul>
<li>The Random Forest (RF) algorithm (Breimann 2001) is a supervised classification algorithm. It builds upon the concept of decision trees (DT).</li>
<li>The RF relies on many self-learning decision trees (i.e.&nbsp;“Forest”). The idea behind using many decision trees (i.e.&nbsp;an ensemble) is that many base learners can come to one strong and robust decision compared to a single DT.</li>
<li>Different from the manual (expert-based) definition of decision rules, the RF uses self-learning decision trees. These trees automatically define rules at each node based on a training dataset.</li>
<li>RF seeks to minimize the heterogeneity of the two resulting subsets of the data created by the respective rule. Heterogeneity is in this case expressed as the Gini impurity index and the rule creating the least heterogeneous subsets of the data is used for the respective node.</li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image21.jpg" width="640"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="accuracy-assessment-for-classifications">Accuracy assessment for classifications</h3>
<p>The basic principle for all accuracy assessment is to compare estimates with reality, and to quantify the difference between the two.</p>
<ul>
<li>In the context of remote sensing-based land cover classifications, the ‘estimates’ are the classes mapped for each pixel, and ‘reality’ is the actual land cover in the areas corresponding to each pixel.</li>
</ul>
</section>
<section class="slide level2">

<h3 id="the-confusion-matrix">The confusion matrix</h3>
<p>A confusion matrix (or error matrix) is usually used as the quantitative method of characterising image classification accuracy. It is a table that shows correspondence between the classification result and a reference image. I.e., to create the confusion matrix we need the ground truth data, such as cartographic information, results of manually digitizing an image, field work/ground survey results recorded with a GPS-receiver.</p>

<img data-src="img/image9.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="user-producer-and-overall-accuracy">User, producer, and overall accuracy</h3>
<div style="font-size: 0.8em">
<ul>
<li>The user’s accuracy column shows false positives, or errors of commission, where pixels are incorrectly classified as a known class when they should have been classified as something different. User’s accuracy is also referred to as Type 1 error. The data to compute this error rate is read from the rows of the table. The user’s accuracy is calculated by dividing the total number of classified points that agree with the reference data by the total number of classified points for that class. The Total row shows the number of points that should have been identified as a given class, according to the reference data.</li>
</ul>
</div>

<img data-src="img/image10.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="user-producer-and-overall-accuracy-1">User, producer, and overall accuracy</h3>
<div style="font-size: 0.8em">
<ul>
<li>The producer’s accuracy column shows false negatives, or errors of omission. The producer’s accuracy indicates how accurately the classification results meet the expectation of the creator. Producer’s accuracy is also referred to as Type 2 error. The data to compute this error rate is read in the columns of the table. The producer’s accuracy is calculated by dividing the total number of classified points that agree with reference data by the total number of reference points for that class. These values are false-negative values within the classified results. The Total column shows the number of points that were identified as a given class, according to the classified map.</li>
</ul>
</div>

<img data-src="img/image11.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="user-producer-and-overall-accuracy-2">User, producer, and overall accuracy</h3>
<div style="font-size: 0.8em">
<ul>
<li>The <strong>overall accuracy</strong> answers the following question: ‘What proportion of the map is correctly classified?’, which can often be interpreted simply as ‘how accurate is the map?’. Looking at the values in the diagonal of the confusion matrix in the confusion matrix below, these sum up to 49+40+59=148, out of a total 166 pixels in the validation data set. 148 out of 166 is 89.16%, so based on the validation data we estimate that 89.16% of the map is correctly classified.</li>
</ul>
</div>

<img data-src="img/image12.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="kappa">Kappa</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>KAPPA analysis is a discrete multivariate technique used in accuracy assessments.</p>
<ul>
<li><p>It yields a Khat statistic (an estimate of KAPPA) that is a measure of agreement or accuracy.</p></li>
<li><p>The Khat statistic is computed by using the formula below:</p></li>
</ul>
<p><img data-src="img/image14.jpg"></p>
</div><div class="column" style="width:50%;">
<p>Rating criteria of Kappa statistics</p>
<p><img data-src="img/image15.jpg" width="589"></p>
<p><img data-src="img/image15.jpg" width="2" height="1"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="f1-score">F1 Score</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The goal of a good image analysis is, of course, to have a large number of True Presences, and a small number of False Presences and a small number of False Negatives.</p></li>
<li><p>To quantify how well the image analysis succeeded in this, the value typically calculated is called the F1 score, which is calculated as:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/image16a.jpg" class="quarto-figure quarto-figure-center" width="423"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image16.jpg"></p>
<ul>
<li>The F1 score has the nice property of having values that range from 0 (worst) to 1 (best), which makes it easy to interpret.</li>
</ul>
</div></div>
</section>
<section id="reference" class="slide level2">
<h2>Reference</h2>
<ul>
<li><p><a href="https://natural-resources.canada.ca/maps-tools-publications/satellite-elevation-air-photos/land-cover-land-use">Land Cover &amp; Land Use</a></p></li>
<li><p>Richards and Jia (2006) <strong>Remote Sensing Digital Image Analysis: An Introduction</strong>, Springer.</p></li>
</ul>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>