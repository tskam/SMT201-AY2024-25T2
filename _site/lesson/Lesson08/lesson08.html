<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Dr.&nbsp;Kam Tin Seong Assoc. Professor of Information Systems">
  <meta name="dcterms.date" content="2025-03-23">
  <title>SMT201-AY2024-25T2 – Lesson 8: Fundamentals of Remote Sensing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lesson 8: Fundamentals of Remote Sensing</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;Kam Tin Seong<br>Assoc. Professor of Information Systems 
</div>
        <p class="quarto-title-affiliation">
            School of Computing and Information Systems,<br>Singapore Management University
          </p>
    </div>
</div>

  <p class="date">2025-03-23</p>
</section>
<section id="content" class="slide level2">
<h2>Content</h2>
<ul>
<li>Principles of Remote Sensing
<ul>
<li>A historical overview</li>
</ul></li>
<li>Physical Principles of Remote Sensing</li>
<li>Basic principles of remotely sensed data
<ul>
<li>Spectral resolution</li>
<li>Spatial resolution</li>
</ul></li>
<li>Remotely Sensed Data Sources</li>
<li>Urban Applications of Remotely Sensed Data</li>
<li>Digital Image Analysis Methods image
<ul>
<li>processing techniques</li>
</ul></li>
</ul>
</section>
<section id="principles-of-remote-sensing" class="slide level2">
<h2>Principles of Remote Sensing</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Remote sensing refers to the activities of recording, observing, and perceiving (sensing) objects or events in far-away (remote) places.</li>
<li>In remote sensing, the sensors are not in direct contact with the objects or events being observed.</li>
<li>Electromagnetic radiation normally is used as the information carrier in remote sensing.</li>
<li>The output of a remote sensing system is usually an image representing the scene being observed.</li>
<li>A further step of image analysis and interpretation is required to extract useful information from the image.</li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/image1.jpg" class="quarto-figure quarto-figure-center" width="307"></p>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="history-of-remote-sensing">History of remote sensing</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The use of aerial Earth survey has started immediately with the invention of the photographic method.</li>
<li>The first aerial photograph was taken by Gaspard-Félix Tournachon (known by the pseudonym Nadar) in 1858. It was a picture of a village near Paris captured from a balloon at an altitude of 80 m.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image2.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="history-of-remote-sensing-pigeon-photographer-method">History of remote sensing: pigeon photographer method</h3>
<p>In 1907, Julius Neubronner, a pharmacist, invented the pigeon photographer method for aerial photography. Initially, he used carrier pigeons to deliver medicines, and then he decided to make an experiment designing an aluminium chest harness with minicamera for pigeons, which took automatic photos at regular intervals. Subsequently, in 1909, Neubronner sold some of such pigeon photography images turned into postcards.</p>

<img data-src="img/image3.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">

<h3 id="history-of-remote-sensing-aerial-photographs">History of remote sensing: aerial photographs</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Aerial photographs started to be taken from planes in 1909.</li>
<li>Aerial photography was used during the World War I in military intelligence.</li>
<li>The period between the two world wars was marked with the development of methods for civil application of aerial photography, first of all in cartography, geology, agriculture and forestry.</li>
<li>Cameras, films and aircrafts improved along with significant development of stereographic mapping method.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/image4.jpg" class="quarto-figure quarto-figure-center" width="600"></p>
</figure>
</div>
<p>Source: <a href="https://www.theguardian.com/world/gallery/2009/nov/23/secondworldwar-secret-photographs-online">Secret second world war aerial images go online</a></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="history-of-remote-sensing-space">History of remote sensing: space</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Exploration of the outer space was the revolutionary step in the development of remote sensing techniques.</li>
<li>Manned spacecrafts, artificial Earth satellites and orbital stations started to deliver extensively satellite images.</li>
<li>The first photographs of ground surface have been captured by TIROS weather satellite and Mercury 1–4 spacecrafts in 1960–1962.</li>
<li>Image on the right shows the first space photograph taken in 1946. The image captures Mexico.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image5.jpg" width="624"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="history-of-remote-sensing-earth-observation-drone">History of remote sensing: Earth observation drone</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Although drone was first introduced in the early 1970s, civilian applications of drone only gaining attention since 2016.</li>
<li>Earth observation drone for urban analysis, on the other hand, is a very recent area of research.</li>
<li>Visit this <a href="https://medium.com/supervisionearth/satellite-vs-drone-imagery-knowing-the-difference-and-effectiveness-of-supervision-earths-90e98b78777c">link</a> to learn more about drone image versus satellite imagery.</li>
</ul>
</div><div class="column" style="width:50%;">
<p>A 5cm resolution aerial photo take from drone.</p>
<p><img data-src="img/image6.jpg" width="600"></p>
</div></div>
</section>
<section id="physical-principles-of-remote-sensing" class="slide level2">
<h2>Physical Principles of Remote Sensing</h2>
<h3 id="electromagnetic-radiation">Electromagnetic Radiation</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>Electromagnetic spectrum is a system, which “classifies all energy by wavelengths (from short wavelength cosmic energy to long wavelength radio waves) that travel harmonically with constant speed of light” (NASA, 2013).</p>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image7.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="spectrum">Spectrum</h3>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 0.7em">
<ul>
<li>The range of electromagnetic waves with different frequencies is called a spectrum.</li>
<li>The following ranges of electromagnetic radiation such as ultraviolet, visible, infrared, microwave, and radio are used to obtain remote sensing data.
<ul>
<li>Ultraviolet range (0.1–0.38 μm) is used to assess the state of plants and water reservoirs, and determine the expansion of trace gases and ozone in atmosphere.</li>
<li>Visible (0.38–0.74 μm) range and infrared (0.75–1000 μm), which is divided into three types due to its wide range: near-infrared (0.75–1.5 μm), intermediate (1.5–3 μm) and far-infrared (3–1000 μm) radiation. Near-infrared and visible ranges are widely used to obtain images of forest areas.</li>
<li>Thermal range (2.5 μm — 1 mm) provides information on the heat field. It has been shown that the temperature difference can reach several degrees in different types of vegetation, plantations of different densities, composition and age, in the surface layer, at the level of the surface and in the soil.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.7em">
<ul>
<li>Microwave radiation (1 mm — 1 m) range provides information about topographic characteristics of territories and water zones, deposits of moisture in soil and plant leaves, effects of industrial emission on plants.</li>
<li>Radio frequency range (1 m — &gt; 10 km) provides information about the underlying terrain. It allows for analysis of the relief of the territory, identification of hazardous natural processes, such as mudflows, landslides etc. Radar photography is possible under any weather conditions and at any time of the day.</li>
</ul>
<p><img data-src="img/image8.jpg"></p>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="passive-and-active-remote-sensing">Passive and Active Remote Sensing</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>RS system measuring natural emission operates with passive remote sensing. Accordingly, this system can sense only when there is natural emission available:</p>
<ul>
<li>during the day — in the visible range;</li>
<li>during the day and at night — in thermal infrared and microwave range.</li>
</ul>
<p><img data-src="img/image9a.jpg" width="230"></p>
</div><div class="column" style="width:50%;">
<p>Active sensors, on the other hand, provide their own energy source for illumination. Active sensors can be used for examining wavelengths that are not sufficiently provided by the sun, such as microwaves, or to better control the way a target is illuminated. Some examples of active sensors are a laser fluorosensor and a synthetic aperture radar (SAR).</p>
<p><img data-src="img/image9b.jpg" width="149"></p>
</div></div>
</section>
<section id="popular-remotely-sensed-data-landsat" class="slide level2">
<h2>Popular Remotely Sensed Data: Landsat</h2>
<ul>
<li><p>The Landsat program is the longest-running enterprise for the acquisition of satellite imagery of the Earth.</p></li>
<li><p>The first satellite within the program was launched in 1972, the most recent one, Landsat 8, on February 11, 2013.</p></li>
</ul>

<img data-src="img/image10.jpg" class="r-stretch"></section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-landsat-1-5">Spatial and spectral resolution of Landsat 1-5</h3>
<p>Landsat 1 through 5 carried the Landsat <a href="https://en.wikipedia.org/wiki/Multispectral_Scanner" title="Multispectral Scanner">Multispectral Scanner</a> (MSS).</p>

<img data-src="img/image11.jpg" class="r-stretch"></section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-landsat-4-5">Spatial and spectral resolution of Landsat 4-5</h3>
<p>Landsat 4 and 5 carried both the MSS and <a href="https://en.wikipedia.org/wiki/Thematic_Mapper" title="Thematic Mapper">Thematic Mapper</a> (TM) instruments.</p>

<img data-src="img/image12.jpg" class="r-stretch"></section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-landsat-7">Spatial and spectral resolution of Landsat 7</h3>

<img data-src="img/image13.jpg" class="r-stretch"></section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-landsat-8">Spatial and spectral resolution of Landsat 8</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Landsat 8 satellite carries the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS) instruments.</li>
<li>The OLI measures in the visible, near infrared, and shortwave infrared portions (VNIR, NIR, and SWIR) of the spectrum.</li>
<li>The TIRS measures land surface temperature in two thermal bands with a new technology that applies quantum physics to detect heat.</li>
</ul>
</div><div class="column" style="width:&quot;50%;">

</div></div>
</section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-landsat-8-1">Spatial and spectral resolution of Landsat 8</h3>
<p>Landsat 8 images have 15-meter panchromatic and 30-meter multi-spectral spatial resolutions along with two TIRS bands at 100 meter resolution.</p>

<img data-src="img/image14.jpg" class="r-stretch"><p>Source: <a href="https://www.usgs.gov/landsat-missions/landsat-8">Landsat 8, USGS</a></p>
</section>
<section class="slide level2">

<h3 id="acquiring-landsat-data">Acquiring Landsat data</h3>
<p>Landsat data can be acquired from various source. The most popular source is <a href="https://earthexplorer.usgs.gov/">EarthExplorer</a> of USGS.</p>

<img data-src="img/image23.jpg" class="r-stretch"></section>
<section id="popular-remotely-sensed-data-sentinel" class="slide level2">
<h2>Popular Remotely Sensed Data: Sentinel</h2>
<p>Sentinel is the <a href="https://en.wikipedia.org/wiki/Copernicus_Programme">Copernicus Programme</a> satellite constellation conducted by the <a href="https://en.wikipedia.org/wiki/European_Space_Agency">European Space Agency</a>. The Sentinel missions have the following objectives:</p>
<ul>
<li>Sentinel‑1 provides all-weather, day and night radar imaging. The first Sentinel‑1A satellite was successfully launched in 2014, and the second, Sentinel‑1B, in two years — on 25 April 2016.</li>
<li>Sentinel‑2 provides high-resolution optical imaging for land services (e. g. imagery of vegetation, soil and water cover, inland waterways and coastal areas). Sentinel‑2 also provides real-time information for emergency services. The first Sentinel‑2 satellite was successfully launched on 23 June 2015.</li>
<li>Sentinel‑3 provides ocean and global land monitoring services. The first Sentinel‑3A satellite was launched on 16 January 2016.</li>
<li>Sentinel‑4 will be launched in 2023. It is intended to provide data for monitoring of atmospheric composition and operate jointly with a Meteosat Third Generation Satellite.</li>
</ul>
</section>
<section class="slide level2">

<h3 id="spatial-and-spectral-resolution-of-sentinel-2">Spatial and spectral resolution of Sentinel-2</h3>
<p>The Sentinel-2 satellites each carry a single multi-spectral instrument (MSI) with 13 spectral channels in the visible/near infrared (VNIR) and short wave infrared spectral range (SWIR).</p>

<img data-src="img/image15.jpg" class="r-stretch"></section>
<section class="slide level2">

<h3 id="acquiring-sentinel-data">Acquiring Sentinel data</h3>
<p>Sentinal data can be acquired from various source. The most popular source is <a href="https://scihub.copernicus.eu/dhus/#/home">Sentinels Scientific Data Hub</a>.</p>

<img data-src="img/image24.jpg" class="r-stretch"></section>
<section id="fundamentals-of-image-processing" class="slide level2">
<h2>Fundamentals of Image Processing</h2>
<ul>
<li><p>Image Enhancement</p></li>
<li><p>Band Combinations, Ratios and Indices</p></li>
<li><p>Colour Composite Images</p></li>
</ul>
</section>
<section id="image-enhancement" class="slide level2">
<h2>Image enhancement</h2>
<ul>
<li><p>Enhancements are used to make it easier for visual interpretation and understanding of imagery. The advantage of digital imagery is that it allows us to manipulate the digital pixel values in an image.</p></li>
<li><p>Common practices include</p>
<ul>
<li><p>contrast enhancement,</p></li>
<li><p><a href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/spatial-filtering" title="Learn more about spatial filtering from ScienceDirect's AI-generated Topic Pages">spatial filtering</a> and</p></li>
<li><p>density slicing.</p></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<h3 id="contrast-enhancement">Contrast enhancement</h3>
<div style="font-size: 0.9em">
<p>Contrast enhancement or stretching is performed by <a href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/linear-transformation" title="Learn more about linear transformation from ScienceDirect's AI-generated Topic Pages">linear transformation</a> expanding the original range of gray level. In raw imagery, the useful data often populates only a small portion of the available range of digital values (commonly 8 bits or 256 levels). Contrast enhancement involves changing the original values so that more of the available range is used, thereby increasing the contrast between targets and their backgrounds.</p>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 1 before enhancement <img data-src="img/image16a.jpg"></p>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 1 after contrast enhancement <img data-src="img/image16b.jpg"></p>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="image-histogram">Image histogram</h3>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 0.8em">
<ul>
<li><p>The key to understanding contrast enhancements is to understand the concept of an <strong>image histogram</strong>.</p></li>
<li><p>A histogram is a graphical representation of the brightness values that comprise an image. The brightness values (i.e.&nbsp;0-255) are displayed along the x-axis of the graph. The frequency of occurrence of each of these values in the image is shown on the y-axis.</p></li>
<li><p>The simplest type of enhancement is a linear contrast stretch. This involves identifying lower and upper bounds from the histogram (usually the minimum and maximum brightness values in the image) and applying a transformation to stretch this range to fill the full range.</p></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.8em">
<ul>
<li>In our example, the minimum value (occupied by actual data) in the histogram is 84 and the maximum value is 153. These 70 levels occupy less than one-third of the full 256 levels available. A linear stretch uniformly expands this small range to cover the full range of values from 0 to 255. This enhances the contrast in the image with light toned areas appearing lighter and dark areas appearing darker, making visual interpretation much easier.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/image17.jpg" class="quarto-figure quarto-figure-center" width="392"></p>
</figure>
</div>
</div>
</div></div>
</section>
<section id="spatial-filtering" class="slide level2">
<h2>Spatial Filtering</h2>
<ul>
<li><p>Spatial filtering encompasses another set of digital processing functions which are used to enhance the appearance of an image.</p></li>
<li><p>Spatial filters are designed to highlight or suppress specific features in an image based on their spatial frequency.</p></li>
<li><p>Spatial frequency is related to the concept of image texture,</p></li>
<li><p>A common filtering procedure involves moving a ‘window’ of a few pixels in dimension (e.g.&nbsp;3x3, 5x5, etc.) over each pixel in the image, applying a mathematical calculation using the pixel values under that window, and replacing the central pixel with the new value.</p></li>
</ul>
</section>
<section class="slide level2">

<h3 id="the-low-pass-filters-smoothing">The low pass filters (Smoothing)</h3>
<p>Smoothing filters (low – pass) straighten data by reducing local variations and removing the noise. The low pass filter calculates the average value for each neighbouring pixel. The result is that the average of the high and low values of each neighbour will be reduced, which will reduce the data extreme values.</p>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 8 before filtering <img data-src="img/image18a.jpg"></p>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 8 after low pass filters <img data-src="img/image18b.jpg"></p>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="the-high-pass-filters-smoothing">The high pass filters (Smoothing)</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The Sharpness filter accentuates the values comparative difference among neighbours. A high pass filter calculates the sum of Statistics focal length for each cell of the input using a weighted neighbourhood of the core. It highlights the boundaries among features (for example, when a body of water meets the forest), accentuating, thus, the contours among the objects. The high pass filter is called contour improvement filter. The core high pass filter identifies which cells to use in the neighbourhood and their weights.</p>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 8 with high pass filtering <img data-src="img/image18c.jpg"></p>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="edge-detection-filters">Edge detection filters</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The third type of filters relates to the detection of edges of the geographical objects.</p>
</div><div class="column" style="width:50%;">
<div style="font-size: 0.6em">
<p>Landsat Band 8 with edge detection filters <img data-src="img/image18d.jpg"></p>
</div>
</div></div>
</section>
<section id="band-combinations-ratios-and-indices" class="slide level2">
<h2>Band Combinations, Ratios and Indices</h2>
<ul>
<li>Normalized Difference Vegetation Index (NDVI)</li>
<li>Normalized Difference Built-up Index (NDBI)</li>
</ul>
</section>
<section class="slide level2">

<h3 id="normalized-difference-vegetation-index">Normalized Difference Vegetation Index</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The Normalized Difference Vegetation Index (NDVI) (Kriegler et al.&nbsp;1969) quantifies vegetation by measuring the difference between near-infrared (which vegetation strongly reflects) and red light (which vegetation absorbs). The NDVI is given by (Akbar 2019):</p>
<p>NDVI= ( NIR − Red) / (NIR + Red )</p>
<p>where NIR and Red is the surface reflective-values for the near infrared (NIR) and the red (R) spectral bands.</p>
<p>Calculations of NDVI for a given pixel always result in a number that ranges from minus one (-1) to plus one (+1).</p>
</div><div class="column" style="width:50%;">
<p>In general, healthy vegetation (chlorophyll) reflects more near-infrared (NIR) and green light compared to other wavelengths. But it absorbs more red and blue light. When you have high NDVI values, you have healthier vegetation.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/image19.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="normalized-difference-built-up-index-ndbi">Normalized Difference Built-Up Index (NDBI)</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>This index highlights urban areas where there is typically a higher reflectance in the shortwave-infrared (SWIR) region, compared to the near-infrared (NIR) region.</p>
<p>The general formula of NDBI is</p>
<p><strong>NDBI = (SWIR – NIR) / (SWIR + NIR)</strong></p>
<p>whereby NIR and SWIR are the near infrared (NIR) and the Short-Wave infrared (SWIR) bands respectively.</p>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Similar to NDVI, the NDBI value lies between -1 to +1. Negative value of NDBI represent water bodies where as higher value represent build-up areas. NDBI value for vegetation is low.</p></li>
<li><p>Applications include watershed runoff predictions and land-use planning.</p></li>
<li><p>The NDBI was originally developed for use with Landsat TM bands 5 and 4. However, it will work with any multispectral sensor with a SWIR band between 1.55-1.75 µm and a NIR band between 0.76-0.9 µm.</p></li>
</ul>
</div></div>
</section>
<section id="colour-composite-images" class="slide level2">
<h2>Colour Composite Images</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>In displaying a colour composite image, three primary colours (red, green and blue) are used. When these three colours are combined in various proportions, they produce different colours in the visible spectrum. Associating each spectral band (not necessarily a visible band) to a separate primary colour results in a colour composite image.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image20.jpg"></p>
</div></div>
</section>
<section class="slide level2">

<h3 id="true-colour-composite-image">True colour composite image</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>If a multispectral image consists of the three visual primary colour bands (red, green, blue), the three bands may be combined to produce a “true colour” image. In this way, the colours of the resulting colour composite image resemble closely what would be observed by the human eyes.</li>
<li>This band combination is well suited for the analysis of aquatic ecosystems, determining water depth. It is also used to study man-made features.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image21.jpg"></p>
<div style="font-size: 0.8em">
<p>Notice that healthy vegetation appears green, cleared fields are light, unhealthy flora is brown and yellow, roads are grey, and coastlines appear whitish.</p>
</div>
</div></div>
</section>
<section class="slide level2">

<h3 id="flase-colour-composite-image">Flase colour composite image</h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The display colour assignment for any band of a multispectral image can be done in an entirely arbitrary manner. In this case, the colour of a target in the displayed image does not have any resemblance to its actual colour. The resulting product is known as a <strong>false colour composite image</strong>.</li>
<li>Image on the right is a false colour composite image created by using Landset 8 Band 5-4-3.</li>
<li>This combination is very popular and is used for analyzing vegetation, monitoring soil and crops.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="img/image22.jpg"></p>
<div style="font-size: 0.8em">
<p>Notice that vegetation appears in shades of red, urban areas are green-blue, and the soil color varies from dark to light brown. The shades of deep red indicate healthy and/or broadleaf vegetation, while grassy or sparse/shrubby vegetation appears in lighter shades.</p>
</div>
</div></div>
</section>
<section id="reference" class="slide level2">
<h2>Reference</h2>
<ul>
<li>Richards, John A (2013) <strong>Remote Sensing Digital Image Analysis: An Introduction</strong>, Springer.
<ul>
<li><a href="https://link-springer-com.libproxy.smu.edu.sg/chapter/10.1007/978-3-642-30062-2_1">Chapter 1: Sources and Characteristics</a></li>
<li><a href="https://link-springer-com.libproxy.smu.edu.sg/chapter/10.1007/978-3-642-30062-2_4">Chapter 4: Radiometric Enhancement of Images</a></li>
<li><a href="https://link-springer-com.libproxy.smu.edu.sg/chapter/10.1007/978-3-642-30062-2_5">Chapter 5: Geometric Processing and Enhancement: Image Domain Techniques</a></li>
</ul></li>
<li>Canada Centre for Remote Sensing <a href="https://natural-resources.canada.ca/sites/www.nrcan.gc.ca/files/earthsciences/pdf/resource/tutor/fundam/pdf/fundamentals_e.pdf">Fundamentals of Remote Sensing</a>.</li>
</ul>
</section>
<section class="slide level2">

<h3 id="articles">Articles</h3>
<ul>
<li>Alexander Vida (2021) <a href="https://storymaps.arcgis.com/stories/f9f8002d55214b5bba5193777fe1c7f0">“Fundamentals of Remote Sensing”</a></li>
<li><a href="https://earthobservatory.nasa.gov/features/FalseColor/page6.php">How to Interpret Common False Color Images</a></li>
<li><a href="https://earthobservatory.nasa.gov/features/FalseColor">Why is that Forest Red and that Cloud Blue? How to Interpret a False-Color Satellite Image</a></li>
<li><a href="https://public.wmo.int/en/bulletin/space-based-earth-observations-societal-benefit">Space-based Earth observations for societal benefit</a></li>
<li><a href="https://eos.org/opinions/data-for-all-using-satellite-observations-for-social-good">Data for All: Using Satellite Observations for Social Good</a></li>
<li><a href="https://www.satimagingcorp.com/satellite-sensors/geoeye-1/">GeoEye-1 Satellite Sensor</a></li>
<li><a href="https://www.usgs.gov/landsat-missions">Landsat and EROS Center</a></li>
<li><a href="https://gisgeography.com/landsat-8-bands-combinations/">Landsat 8 Bands and Band Combinations</a></li>
<li><a href="https://sentinels.copernicus.eu/web/sentinel/home">Sentinel Online</a></li>
<li><a href="https://gisgeography.com/sentinel-2-bands-combinations/">Sentinel 2 Bands and Combinations</a></li>
</ul>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>